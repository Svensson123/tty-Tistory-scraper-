#!/usr/bin/env python

import urllib.request
import urllib.parse
import threading
import logging
import hashlib
import queue
import sys
import os
import tistory_extractor as tistory
import httpbin
import argparser

# issue: if multiple pages have the same url saved to same directory they'll be marked as "already saved"
# fix title_filter

CONTENT_TYPES = ["image/jpeg", "image/png", "image/gif, image/webp"]
IMG_EXTS = ['jpg', 'jpeg', 'png', 'gif', 'webp']
LOCK = threading.Lock()
SETTINGS = argparser.parse(sys.argv[1:])
SAVED = 0
EXISTING = 0


def run():
    if SETTINGS.debug_status():
        logging.basicConfig(level=logging.DEBUG, format='%(name)s: %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(message)s')

    logging.debug('%s', SETTINGS.get_url())

    pic_q = queue.Queue()

    # Parse pages for images
    if SETTINGS.multiplepages():
        page_q = queue.Queue()
        for page in SETTINGS.get_pages():
            page_q.put(page)
        logging.info("Fetching source for:")
        start_threads(parse_multiple_pages, [page_q, pic_q], SETTINGS.get_threads())
    else:
        logging.info("Fetching page source...")
        html = httpbin.Fetch(SETTINGS.get_url()).body()
        if html:
            parse_page(SETTINGS.get_url(), pic_q)
        else:
            sys.exit()
    total_img_found = pic_q.qsize()

    # Starts the download
    logging.info("\nStarting download:")
    start_threads(download, [pic_q], SETTINGS.get_threads())

    # Final report
    logging.info("\nDone!")
    logging.info(f"Found: {total_img_found}")
    if SAVED > 0:
        logging.info(f"Saved: {SAVED}")
    if EXISTING > 0:
        logging.info(f"Already saved: {EXISTING}")

    if httpbin.Fetch.errors:
        logging.info("\nCould not download:")
        for url in httpbin.Fetch.errors:
            logging.info(url)


def start_threads(target, q, n):
    img_threads = [threading.Thread(target=target, args=q) for i in range(n)]
    for thread in img_threads:
        thread.start()
    for thread in img_threads:
        thread.join()


def download(pic_q):
    global SAVED
    global EXISTING

    while pic_q.qsize() > 0:
        data = pic_q.get()
        url = data["url"]
        title = data["title"]

        content = httpbin.Fetch(url)
        if not content:
            continue

        logging.info(url)
        with LOCK:
            filename = check_filename(data['filename'], content.info(), url)
            img_path = get_img_path(title, content, filename)
            if SETTINGS.debug:
                continue
            if img_path:
                try:
                    with open(img_path, 'wb') as f:
                        f.write(content.body())
                except Exception as err:
                    logging.info('Error: %s', err)

                SAVED += 1
            else:
                EXISTING += 1


def get_img_path(title, content, filename):
    extension = "." + content.info()["Content-Type"].split("/")[1]
    extension = extension.replace("jpeg", "jpg").strip()

    if '.' in filename and filename.rsplit('.', 1)[1] not in IMG_EXTS:
        # if filename randomly has a dot in its name
        filename = filename + extension
    elif '.' not in filename:
        # no filename has no extension
        filename = filename + extension

    img_path = get_path(title, filename)

    for _ in range(999):
        if not os.path.exists(img_path):
            return img_path

        existing_file = open(img_path, 'rb').read()

        if same_file(existing_file, content.body()):
            break
        else:
            number = filename[filename.rfind("(") + 1:filename.rfind(")")]

            if number.isdigit() and filename.rsplit(".", 1)[1].lower() in IMG_EXTS:
                file_number = int(number) + 1
                filename = filename.rsplit("(", 1)[0]
            else:
                file_number = 2
                filename = filename.rsplit(".", 1)[0]

            filename = f"{filename} ({file_number}){extension}"

            img_path = get_path(title, filename)

    return None


def same_file(file1, file2):
    hash1 = hashlib.md5(file1)
    hash2 = hashlib.md5(file2)

    if hash1.digest() == hash2.digest():
        return True

    return False


def check_filename(filename, img_info, url):
    if img_info['Content-Disposition'] and not filename:
        # filename fallback 1
        filename = img_info['Content-Disposition']
        if "filename*=UTF-8" in filename:
            filename = filename.split("filename*=UTF-8''")[1]
            filename = filename.rsplit(".", 1)[0]
        else:
            filename = filename.split('"')[1]
        filename = urllib.request.url2pathname(filename)

    if not filename:
        # filename fallback 2
        filename = url.rsplit('/', 1)[1]
        filename = filename.strip('/')

    if not filename:
        # filename fallback 3
        filename = 'image'

    filename = filename.strip()
    return filename


def get_path(title, file):
    path = ''
    if SETTINGS.directory:
        path = SETTINGS.directory

    if SETTINGS.organize:
        path = os.path.join(path, title.strip())
        if not os.path.exists(path):
            os.makedirs(path)

    path = os.path.join(path, file)
    return path


def parse_multiple_pages(page_q, pic_q):
    while page_q.qsize() > 0:
        page_num = page_q.get()
        url = "{}{}".format(SETTINGS.get_url(), page_num)
        logging.info(url)
        parse_page(url, pic_q, page_num)


def parse_page(url, pic_q, page_num=''):
    html = httpbin.Fetch(url).body()
    if html:
        page = tistory.Extractor(url, html)
        for link in page.get_links():
            pic_q.put(link)


if __name__ == "__main__":
    # 'ty https://ohcori.tistory.com/ --debug -p 301 305 -o -t 2 -f hello/world'.split()
    run()
